{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization\n",
    "\n",
    "\n",
    "### Definitions\n",
    "\n",
    "Let $\\mathcal{X}=(\\textbf{x}_1, \\textbf{x}_2, ..., \\textbf{x}_n)$ be a sample ($\\textbf{x}_i\\in \\mathbb{R}^d$) of $n$ i.i.d. observations from a mixture of two distinct $d$-dimensional multivariate Gaussian distributions. Let $\\mathcal{Y} = (y_1, y_2, ..., y_n)$ be the set of group labels, that is $y\\in\\{1,2\\}$ which indicate from which Gaussian mixture each observation $\\textbf{x}_i$ was truly sampled from. The set $\\mathcal{Y}$ is sometimes called \"latent\" indicating that the gourp labels $y_i$ are unknown beforehand. That is, we do not know into which Gaussians the samples belong to. Furthermore, we define $\\boldsymbol\\theta = (\\boldsymbol\\tau, \\boldsymbol\\mu_1, \\boldsymbol\\mu_2, \\boldsymbol\\Sigma_1, \\boldsymbol\\Sigma_2)$ to be a set of parameters which fully describe the distributions of the data. The parameters $\\boldsymbol\\tau = (\\tau_1, \\tau_2)$ are the \"mixing parameters\" with $\\tau_1+\\tau_2 = 1$ which determine the weights of \"how much\" each sample $\\textbf{x}_i$ came from either Gaussian. We treat these \"mixing values\" as the prior probabilities that a given sample $\\textbf{x}_i$ \"belongs\" to either Gaussian 1 or Gaussian 2. Explicitly put, we state this as $P(y_i=1|\\boldsymbol\\theta)=\\tau_1$ and $P(y_2=1|\\boldsymbol\\theta)=\\tau_2=1-\\tau_1$.\n",
    "\n",
    "If sample $\\textbf{x}_i$ was generated by Gaussian 1, then $\\textbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1)$ and if it was generated by Gaussian 2, then $\\textbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2)$. Explicitly put: \n",
    "\n",
    "$$p(\\textbf{x}_i|y_i=1, \\boldsymbol\\theta)=\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_1|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_1)^T\\boldsymbol\\Sigma^{-1}_1(\\textbf{x}_i-\\boldsymbol\\mu_1)\\right),\\;\\;\\;\\text{if $\\textbf{x}_i$ is known to be generated by Gaussian 1}$$\n",
    "\n",
    "$$p(\\textbf{x}_i|y_i=2, \\boldsymbol\\theta)=\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_2|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_2)^T\\boldsymbol\\Sigma^{-1}_2(\\textbf{x}_i-\\boldsymbol\\mu_2)\\right),\\;\\;\\;\\text{if $\\textbf{x}_i$ is known to be generated by Gaussian 2}.$$\n",
    "\n",
    "\n",
    "If $\\textbf{x}_i$ is generated by the mixture of these two Gaussians, then the probability density of $\\textbf{x}_i$ is: \n",
    "\n",
    "$$p(\\textbf{x}_i|\\boldsymbol\\theta) = \\sum_{j=1}^2 p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta) = \\sum_{j=1}^2 P(y_i=j|\\boldsymbol\\theta)\\,p(\\textbf{x}_i|y_i=j, \\boldsymbol\\theta)=\\tau_1 \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_1|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_1)^T\\boldsymbol\\Sigma^{-1}_1(\\textbf{x}_i-\\boldsymbol\\mu_1)\\right) + \\tau_2 \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_2|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_2)^T\\boldsymbol\\Sigma^{-1}_2(\\textbf{x}_i-\\boldsymbol\\mu_2)\\right),$$\n",
    "\n",
    " If we integrate the mixture density of $\\textbf{x}_i$:\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} p(\\textbf{x}_i|\\boldsymbol\\theta)\\,d\\textbf{x} = \\int_{-\\infty}^{\\infty}\\sum_{j=1}^2 P(y_i=j|\\boldsymbol\\theta)\\,p(\\textbf{x}_i|y_i=j, \\boldsymbol\\theta)\\,d\\textbf{x}= \\tau_1 \\underbrace{\\int_{-\\infty}^{\\infty} p(\\textbf{x}_i|y_i=1, \\boldsymbol\\theta)\\,d\\textbf{x}}_{= 1} + \\tau_2 \\underbrace{\\int_{-\\infty}^{\\infty} p(\\textbf{x}_i|y_i=2, \\boldsymbol\\theta)\\,d\\textbf{x}}_{= 1} = \\tau_1 + \\tau_2 = 1$$\n",
    "\n",
    "as it should. \n",
    "\n",
    "### Incomplete data likelihood\n",
    "\n",
    "Let us first define the \"incomplete\" data likelihood (with symbol $L$ as likelihood, since it's not the same as probability): \n",
    "\n",
    "$$L(\\mathcal{X}|\\boldsymbol\\theta)=L(\\textbf{x}_1, \\textbf{x}_2, ..., \\textbf{x}_n|\\boldsymbol\\theta) = \\prod_{i=1}^n p(\\textbf{x}_i|\\boldsymbol\\theta) = \\prod_{i=1}^n \\sum_{j=1}^2 P(y_i=j|\\boldsymbol\\theta) \\, p(\\textbf{x}_i|y_i=j, \\boldsymbol\\theta) = \\prod_{i=1}^n \\sum_{j=1}^2 \\tau_j \\, p(\\textbf{x}_i|y_i=j, \\boldsymbol\\theta),$$\n",
    "\n",
    "This \"incomplete data likelihood\" means the likelihood of observing the data set $\\mathcal{X}$ prior to not having any information about the labels in $\\mathcal{Y}$ assuming that both Gaussians contribute in the generation of the samples. \n",
    "\n",
    "### Complete data likelihood \n",
    "\n",
    "The \"complete data likelihood\" is the likelihood of observing the pair $(\\mathcal{X}, \\mathcal{Y})$ from the Gaussian mixture. In this case, since the set $\\mathcal{Y}$ is given we know for sure that either Gaussian 1 or Gaussian 2 completely generated a given sample $\\textbf{x}_i$. Thus we can write the complete data likelihood as:   \n",
    "\n",
    "$$L(\\mathcal{X}, \\mathcal{Y}|\\boldsymbol\\theta)=L(\\textbf{x}_1, \\textbf{x}_2, ..., \\textbf{x}_n, y_1, y_2, ..., y_n|\\boldsymbol\\theta) = \\prod_{i=1}^n p(\\textbf{x}_i, y_i|\\boldsymbol\\theta) = \\prod_{i=1}^n \\sum_{j=1}^2 P(y_i=j|\\boldsymbol\\theta) \\, p(\\textbf{x}_i|y_i=j,\\boldsymbol\\theta)\\, \\mathbb{I}(y_i=j) = \\prod_{i=1}^n \\sum_{j=1}^2 \\tau_j \\, p(\\textbf{x}_i|y_i=j,\\boldsymbol\\theta)\\, \\mathbb{I}(y_i=j),$$\n",
    "\n",
    "where $\\mathbb{I}(y_i=j)\\in\\{0,1\\}$ is the indicator function. Why is the indicator function added here? Lets take a look at the joint density above, that is $p(\\textbf{x}_i, y_i|\\boldsymbol\\theta)$. What is this function saying? If we did not care from which Gaussian the observation $\\textbf{x}_i$ was sampled from, then the density of $\\textbf{x}_i$ would be described by $p(\\textbf{x}_i|\\boldsymbol\\theta) = \\sum_{j=1}^2 p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta)$, that is by the combination of the two Gaussians. Now, with the function $p(\\textbf{x}_i, y_i|\\boldsymbol\\theta)$ the density depends on from which Gaussian $\\textbf{x}_i$ was generated from. Thus if $y_i=1$, then the \"part\" of the mixture density belonging to Gaussian 2 had no probabilistic effect on the sampling of $\\textbf{x}_i$ and thus only the density caused by Gaussian 1 had a part to play.  \n",
    "\n",
    "Lets open up the incomplete likelihood more: \n",
    "\n",
    "$$L(\\mathcal{X}, \\mathcal{Y}|\\boldsymbol\\theta)=\\prod_{i=1}^n \\sum_{j=1}^2 \\tau_j \\, p(\\textbf{x}_i|y_i=j,\\boldsymbol\\theta)\\, \\mathbb{I}(y_i=j) = \\prod_{i=1}^n \\sum_{j=1}^2 \\exp\\left(\\ln \\tau_j\\right) \\, \\exp\\left(\\ln\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}\\right)\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j)\\right)\\, \\mathbb{I}(y_i=j)$$\n",
    "\n",
    "$$=\\prod_{i=1}^n \\sum_{j=1}^2 \\exp\\left(\\ln \\tau_j + \\ln\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j) \\right)\\,\\mathbb{I}(y_i=j)$$\n",
    "\n",
    "At this point, lets take a look at the inner sum:\n",
    "\n",
    "$$\\sum_{j=1}^2 \\exp\\left(\\ln \\tau_j + \\ln\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j) \\right)\\,\\mathbb{I}(y_i=j)$$\n",
    "\n",
    "$$=\\exp\\left(\\ln \\tau_1 + \\ln\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_1|}}-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_1)^T\\boldsymbol\\Sigma^{-1}_1(\\textbf{x}_i-\\boldsymbol\\mu_1) \\right)\\,\\mathbb{I}(y_i=1)+\\exp\\left(\\ln \\tau_2 + \\ln\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_2|}}-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_2)^T\\boldsymbol\\Sigma^{-1}_2(\\textbf{x}_i-\\boldsymbol\\mu_2) \\right)\\, \\mathbb{I}(y_i=2).$$\n",
    "\n",
    "Because $\\mathbb{I}(y_i=j)$ equals either $0$ or $1$ we know for sure that one of the above terms is always zero. Getting back to the complete likelihood, this is why it is true that: \n",
    "\n",
    "$$p(\\mathcal{X}, \\mathcal{Y}|\\boldsymbol\\theta)=\\prod_{i=1}^n \\underbrace{\\sum_{j=1}^2 \\exp\\left(\\ln \\tau_j + \\ln\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j) \\right)\\,\\mathbb{I}(y_i=j)}_{\\text{Only one term in this sum}}=\\exp\\left(\\sum_{i=1}^n \\sum_{j=1}^2 \\left\\{\\ln \\tau_j + \\ln\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j) \\right\\}\\, \\mathbb{I}(y_i=j)\\right)\\,$$\n",
    "\n",
    "$$=\\exp\\left(\\sum_{i=1}^n \\sum_{j=1}^2 \\left\\{\\ln \\tau_j - \\frac{d}{2}\\ln 2\\pi - \\frac{1}{2}\\ln |\\boldsymbol\\Sigma_j|-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j) \\right\\}\\, \\mathbb{I}(y_i=j)\\right),$$\n",
    "\n",
    "and furthermore, since only one of the functions $\\mathbb{I}(y_i=j)$ equals 1 and the rest are zero, we can safely omit the indicator from the equation without affecting the results. Thus: \n",
    "\n",
    "$$p(\\mathcal{X}, \\mathcal{Y}|\\boldsymbol\\theta)=\\exp\\left(\\sum_{i=1}^n \\sum_{j=1}^2 \\left\\{\\ln \\tau_j - \\frac{d}{2}\\ln 2\\pi - \\frac{1}{2}\\ln |\\boldsymbol\\Sigma_j|-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j) \\right\\}\\right).$$\n",
    "\n",
    "As a next step, let us look at the density $p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta)$. By applying the Bayes theorem (where distributions can be discrete and continuous) we have that: \n",
    "\n",
    "$$p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta) = \\frac{p(\\textbf{x}_i|\\,y_i=j, \\boldsymbol\\theta)P(y_i=j|\\,\\boldsymbol\\theta)}{p(\\textbf{x}_i|\\,\\boldsymbol\\theta)}=\\frac{p(\\textbf{x}_i|\\,y_i=j, \\boldsymbol\\theta)\\,\\tau_j}{\\sum_{k=1}^2 p(\\textbf{x}_i, y_i = k |\\, \\boldsymbol\\theta)}=\\frac{p(\\textbf{x}_i|\\,y_i=j, \\boldsymbol\\theta)\\,\\tau_j}{\\sum_{k=1}^2 p(\\textbf{x}_i|\\,y_i=k, \\boldsymbol\\theta)\\,\\tau_k} = \\frac{\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j)\\right)\\,\\tau_j}{\\sum_{k=1}^2 \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_k|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k)\\right)\\,\\tau_k}$$\n",
    "\n",
    "### Finding the MLE solution\n",
    "\n",
    "Next, let us define the function: \n",
    "\n",
    "$$Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)}) = E_{Y|X,\\boldsymbol\\theta^{(t)}}\\left[\\ln p(\\mathcal{X}, \\mathcal{Y}|\\boldsymbol\\theta)\\right],$$\n",
    "\n",
    "where $X$ and $Y$ refer to the random variables of $\\textbf{x}$ and $y$, and $\\boldsymbol\\theta^{(t)}$ referes to the parameter set $\\boldsymbol\\theta$ at (time) step $t$, that is $\\boldsymbol\\theta^{(t)} = (\\boldsymbol\\tau^{(t)}, \\boldsymbol\\mu_1^{(t)}, \\boldsymbol\\mu_2^{(t)}, \\boldsymbol\\Sigma_1^{(t)}, \\boldsymbol\\Sigma_2^{(t)})$. Let us continue with this function:\n",
    "\n",
    "$$Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)}) = E_{Y|X,\\boldsymbol\\theta^{(t)}}\\left[\\ln p(\\mathcal{X}, \\mathcal{Y}|\\boldsymbol\\theta)\\right]=E_{Y|X,\\boldsymbol\\theta^{(t)}}\\left[\\ln \\prod_{i=1}^n p(\\textbf{x}_i, y_i|\\boldsymbol\\theta)\\right]=E_{Y|X,\\boldsymbol\\theta^{(t)}}\\left[ \\sum_{i=1}^n \\ln p(\\textbf{x}_i, y_i|\\boldsymbol\\theta)\\right]= \\sum_{i=1}^n E_{Y|X,\\boldsymbol\\theta^{(t)}}\\left[ \\ln p(\\textbf{x}_i, y_i|\\boldsymbol\\theta)\\right] =\\sum_{i=1}^n \\sum_{j=1}^2 p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\ln p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta).$$\n",
    "\n",
    "#### The mixing parameters\n",
    "\n",
    "Let us first find the optimal mixing parameters. To do so, recall that we had the constraint $\\tau_1 + \\tau_2 = 1$ so we need to use Langrange multipliers and thus our optimization function becomes: \n",
    "\n",
    "$$f(\\lambda, \\boldsymbol\\tau) = Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)}) + \\lambda(\\tau_1+\\tau_2-1),$$\n",
    "\n",
    "and thus by taking the derivative, setting to zero, etc.: \n",
    "\n",
    "$$\\frac{\\partial f(\\lambda, \\boldsymbol\\tau)}{\\partial \\tau_k} = \\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\tau_k} + \\lambda = 0 \\Leftrightarrow \\lambda = - \\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\tau_k}.$$\n",
    "\n",
    "So what is $\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\tau_k}$? Lets calculate it: \n",
    "\n",
    "$$\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\tau_k} = \\frac{\\partial}{\\partial\\tau_k}\\left(\\sum_{i=1}^n \\sum_{j=1}^2 p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\ln p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta)\\right)=\\underbrace{\\frac{\\partial}{\\partial\\tau_k}\\left(\\sum_{i=1}^n \\sum_{j=1}^2 \\color{red}{p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})} \\ln p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta)\\right)}_{\\text{Notice that $p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})$ is a function of the old $\\boldsymbol\\theta^{(t)}$ parameters}}= \\sum_{i=1}^n \\sum_{j=1}^2 p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\tau_k}\\left( \\ln p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta) \\right).$$\n",
    "\n",
    "Lets look at the partial derivative of the log function: \n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\tau_k}\\left( \\ln p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta) \\right) = \\frac{\\partial}{\\partial\\tau_k}\\left( \\ln P(y_i=j|\\boldsymbol\\theta)\\, p(\\textbf{x}_i|y_i=j, \\boldsymbol\\theta) \\right) = \\frac{\\partial}{\\partial\\tau_k}\\left( \\ln\\left\\{ \\tau_j \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j)\\right)\\right\\} \\right)$$\n",
    "\n",
    "$$=\\frac{\\partial}{\\partial\\tau_k}\\left( \\ln\\tau_j + \\ln\\left\\{ \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j)\\right)\\right\\} \\right) = \\frac{\\partial}{\\partial\\tau_k}(\\ln\\tau_j) + \\underbrace{\\frac{\\partial}{\\partial\\tau_k}\\left( \\ln\\left\\{ \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_j|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_j)^T\\boldsymbol\\Sigma^{-1}_j(\\textbf{x}_i-\\boldsymbol\\mu_j)\\right)\\right\\}\\right)}_{\\text{Does not depend on $\\tau_k$ so equals 0}} = \\frac{\\partial}{\\partial\\tau_k}(\\ln\\tau_j) = \\frac{1}{\\tau_k}, \\;\\;\\;\\text{when $k=j$ and $0$ otherwise}.$$\n",
    "\n",
    "Thus we have: \n",
    "\n",
    "$$\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\tau_k} = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{1}{\\tau_k},$$\n",
    "\n",
    "from which it follows: \n",
    "\n",
    "$$\\lambda = - \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{1}{\\tau_k} \\Leftrightarrow \\lambda\\tau_k = - \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}).$$\n",
    "\n",
    "Let us now sum both sides for $k=1,2$ and we get: \n",
    "\n",
    "$$\\lambda\\underbrace{\\sum_{k=1}^2 \\tau_k}_{=1} = - \\sum_{i=1}^n \\underbrace{\\sum_{k=1}^2 p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})}_{=1} \\Leftrightarrow \\lambda= - \\sum_{i=1}^n 1 \\Leftrightarrow \\lambda = -n.$$\n",
    "\n",
    "Subsituting the $\\lambda=-n$ to the previous equation we get: \n",
    "\n",
    "$$\\lambda\\tau_k = - \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\Leftrightarrow -n\\tau_k = - \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\Leftrightarrow \\tau_k = \\frac1n\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}).$$\n",
    "\n",
    "Thus we get that optimal next time step parameter for $\\tau_k$ (which we denote by $\\tau_k^{(t+1)}$) is:\n",
    "\n",
    "$$ \\boxed{\\tau_k^{(t+1)} = \\frac1n\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) = \\frac1n\\sum_{i=1}^n \\frac{\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma^{(t)}_k|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_k)^T(\\boldsymbol\\Sigma^{(t)}_k)^{-1}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_k)\\right)\\,\\tau^{(t)}_k}{\\sum_{j=1}^2 \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma^{(t)}_j|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_j)^T(\\boldsymbol\\Sigma^{(t)}_j)^{-1}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_j)\\right)\\,\\tau^{(t)}_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean parameters\n",
    "\n",
    "Next, let us find the optimal $\\boldsymbol\\mu_k = (\\mu_{k1}, \\mu_{k2}, ..., \\mu_{kd})$:\n",
    "\n",
    "$$\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\mu_{kp}} = \\frac{\\partial}{\\partial\\mu_{kp}}\\left(\\sum_{i=1}^n \\sum_{j=1}^2 p(y_i=j|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\ln p(\\textbf{x}_i, y_i=j|\\boldsymbol\\theta)\\right) = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left( \\ln p(\\textbf{x}_i, y_i=k|\\boldsymbol\\theta)\\right) = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\frac{\\partial}{\\partial\\mu_{kp}}\\left( \\ln P(y_i=k|\\boldsymbol\\theta)\\, p(\\textbf{x}_i|y_i=k, \\boldsymbol\\theta) \\right) = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\frac{\\partial}{\\partial\\mu_{kp}}\\left( \\ln\\left\\{ \\tau_k \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_k|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k)\\right)\\right\\} \\right) = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left( \\ln\\tau_k + \\ln\\left\\{ \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_k|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k)\\right)\\right\\} \\right)$$\n",
    "\n",
    "\n",
    "$$= \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left( \\ln \\tau_k - \\frac{d}{2}\\ln 2\\pi - \\frac{1}{2}\\ln |\\boldsymbol\\Sigma_k|-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k) \\right) $$\n",
    "\n",
    "$$= \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k) \\right)$$\n",
    "\n",
    "$$=-\\frac{1}{2} \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left(\\textbf{x}_i^T\\boldsymbol\\Sigma^{-1}_k\\textbf{x}_i - \\boldsymbol\\mu_k^T\\boldsymbol\\Sigma^{-1}_k\\textbf{x}_i - \\textbf{x}_i^T\\boldsymbol\\Sigma^{-1}_k\\boldsymbol\\mu_k + \\boldsymbol\\mu_k^T\\boldsymbol\\Sigma^{-1}_k\\boldsymbol\\mu_k\\right)$$\n",
    "\n",
    "$$=-\\frac{1}{2} \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left(- \\boldsymbol\\mu_k^T\\boldsymbol\\Sigma^{-1}_k\\textbf{x}_i - \\textbf{x}_i^T\\boldsymbol\\Sigma^{-1}_k\\boldsymbol\\mu_k + \\boldsymbol\\mu_k^T\\boldsymbol\\Sigma^{-1}_k\\boldsymbol\\mu_k\\right),$$\n",
    "\n",
    "at this point let us denote $\\boldsymbol\\Sigma^{-1}_k = \\begin{pmatrix} a_{11} & \\cdots & a_{1d} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{d1} & \\cdots & a_{dd}\\end{pmatrix} = [\\textbf{a}_1, \\textbf{a}_2, ..., \\textbf{a}_d]$, where $\\textbf{a}_j = \\begin{pmatrix} a_{1j} \\\\ \\vdots \\\\ a_{dj}\\end{pmatrix}$ and continue: \n",
    "\n",
    "$$\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\mu_{kp}}=-\\frac{1}{2} \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left[- \\boldsymbol\\mu_k^T\\begin{pmatrix} \\textbf{a}_1^T\\textbf{x}_i \\\\ \\vdots \\\\ \\textbf{a}_d^T\\textbf{x}_i\\end{pmatrix} - \\textbf{x}_i^T\\begin{pmatrix} \\textbf{a}_1^T\\boldsymbol\\mu_k \\\\ \\vdots \\\\ \\textbf{a}_d^T\\boldsymbol\\mu_k\\end{pmatrix} + \\boldsymbol\\mu_k^T\\begin{pmatrix} \\textbf{a}_1^T\\boldsymbol\\mu_k \\\\ \\vdots \\\\ \\textbf{a}_d^T\\boldsymbol\\mu_k\\end{pmatrix}\\right]$$\n",
    "\n",
    "$$=-\\frac{1}{2} \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial\\mu_{kp}}\\left[- 2\\mu_{kp}\\textbf{x}_i^T\\textbf{a}_p + \\mu_{kp}\\boldsymbol\\mu_k^T\\textbf{a}_p\\right]  =-\\frac{1}{2} \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\left[- 2\\textbf{x}_i^T\\textbf{a}_p + 2\\boldsymbol\\mu_k^T\\textbf{a}_p\\right]$$\n",
    "\n",
    "$$ = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,\\textbf{a}_p^T(\\textbf{x}_i-\\boldsymbol\\mu_k).$$\n",
    "\n",
    "Setting this to zero we get: \n",
    "\n",
    "$$\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,\\textbf{a}_p^T(\\textbf{x}_i-\\boldsymbol\\mu_k)= 0,$$\n",
    "\n",
    "and noting that we can group all the optimal $\\mu_{kp}$ values into vector equation as: \n",
    "\n",
    "$$\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k)= \\boldsymbol0,$$\n",
    "\n",
    "and thus: \n",
    "\n",
    "$$\\boldsymbol\\Sigma_k\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k)= \\boldsymbol\\Sigma_k\\boldsymbol0 \\Leftrightarrow \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,\\boldsymbol\\Sigma_k\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k)= \\boldsymbol0 \\Leftrightarrow \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,(\\textbf{x}_i-\\boldsymbol\\mu_k)= \\boldsymbol0,$$\n",
    "\n",
    "from which we get that the next step optimal mean vector $\\boldsymbol\\mu_k^{(t+1)}$ is \n",
    "\n",
    "$$ \\boxed{\\boldsymbol\\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,\\textbf{x}_i}{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})}},$$\n",
    "\n",
    "thus with the components: \n",
    "\n",
    "$$ \\boxed{\\mu_{kp}^{(t+1)} = \\frac{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})\\,x_{ip}}{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})}},$$\n",
    "\n",
    "where: \n",
    "\n",
    "$$ p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) = \\frac{\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma^{(t)}_k|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_k)^T(\\boldsymbol\\Sigma^{(t)}_k)^{-1}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_k)\\right)\\,\\tau^{(t)}_k}{\\sum_{j=1}^2 \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma^{(t)}_j|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_j)^T(\\boldsymbol\\Sigma^{(t)}_j)^{-1}(\\textbf{x}_i-\\boldsymbol\\mu^{(t)}_j)\\right)\\,\\tau^{(t)}_j}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The covariance parameters\n",
    "\n",
    "Denote $\\boldsymbol\\Sigma_k = \\begin{pmatrix} s_{11}^k & \\cdots & s_{1d}^k \\\\ \\vdots & \\ddots & \\vdots \\\\ s_{d1}^k & \\cdots & s_{dd}^k\\end{pmatrix}$\n",
    "\n",
    "$$\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial s_{rp}^k} = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial s_{rp}^k}\\left( \\ln\\tau_k + \\ln\\left\\{ \\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol\\Sigma_k|}}\\exp\\left(-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k)\\right)\\right\\} \\right)$$\n",
    "\n",
    "$$= \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial s_{rp}^k}\\left( \\ln \\tau_k - \\frac{d}{2}\\ln 2\\pi - \\frac{1}{2}\\ln |\\boldsymbol\\Sigma_k|-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k) \\right)$$\n",
    "\n",
    "$$= \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial s_{rp}^k}\\left(- \\frac{1}{2}\\ln |\\boldsymbol\\Sigma_k|-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k) \\right).$$\n",
    "\n",
    "At this point I will list few matrix differentiation rules which state (you can check these up from matrix differentiation literature, e.g. [wiki](https://en.wikipedia.org/wiki/Matrix_calculus)): \n",
    "\n",
    "$$\\frac{\\partial \\ln |A|}{\\partial A_{ij}} = A_{ij}^{-1}  \\rightarrow \\frac{\\partial \\ln |A|}{\\partial A} = A^{-1}$$\n",
    "$$\\frac{\\partial \\textbf{x}^T A\\textbf{x}}{\\partial A_{ij}} = -(A^{-1}\\textbf{x}\\textbf{x}^T A^{-1})_{ij}  \\rightarrow \\frac{\\partial \\textbf{x}^T A\\textbf{x}}{\\partial A} = -A^{-1}\\textbf{x}\\textbf{x}^T A^{-1},$$\n",
    "\n",
    "where I have used the subscripts $()_{ij}$ to denote the $ij$th element of the corresponding matrix. Using these results we get: \n",
    "\n",
    "$$\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial s_{rp}^k} =\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial s_{rp}^k}\\left(- \\frac{1}{2}\\ln |\\boldsymbol\\Sigma_k|-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k) \\right)=\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\left(- \\frac{1}{2}(\\boldsymbol\\Sigma_k^{-1})_{rp}+\\frac{1}{2}\\left(\\boldsymbol\\Sigma_k^{-1}(\\textbf{x}_i-\\boldsymbol\\mu_k)(\\textbf{x}_i-\\boldsymbol\\mu_k)^T \\boldsymbol\\Sigma_k^{-1}\\right)_{rp} \\right)$$\n",
    "\n",
    "$$\\Leftrightarrow \\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\boldsymbol\\Sigma_k} =\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\frac{\\partial}{\\partial \\boldsymbol\\Sigma_k}\\left(- \\frac{1}{2}\\ln |\\boldsymbol\\Sigma_k|-\\frac{1}{2}(\\textbf{x}_i-\\boldsymbol\\mu_k)^T\\boldsymbol\\Sigma^{-1}_k(\\textbf{x}_i-\\boldsymbol\\mu_k) \\right)=\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\left(- \\frac{1}{2}\\boldsymbol\\Sigma_k^{-1}+\\frac{1}{2}\\boldsymbol\\Sigma_k^{-1}(\\textbf{x}_i-\\boldsymbol\\mu_k)(\\textbf{x}_i-\\boldsymbol\\mu_k)^T \\boldsymbol\\Sigma_k^{-1} \\right)$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})}{\\partial \\boldsymbol\\Sigma_k} =\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\left(- \\frac{1}{2}\\boldsymbol\\Sigma_k^{-1}+\\frac{1}{2}\\boldsymbol\\Sigma_k^{-1}(\\textbf{x}_i-\\boldsymbol\\mu_k)(\\textbf{x}_i-\\boldsymbol\\mu_k)^T \\boldsymbol\\Sigma_k^{-1} \\right) = 0$$\n",
    "\n",
    "$$\\Leftrightarrow \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\boldsymbol\\Sigma_k^{-1} = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\boldsymbol\\Sigma_k^{-1}(\\textbf{x}_i-\\boldsymbol\\mu_k)(\\textbf{x}_i-\\boldsymbol\\mu_k)^T \\boldsymbol\\Sigma_k^{-1}$$\n",
    "\n",
    "$$\\Leftrightarrow \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\boldsymbol\\Sigma_k\\boldsymbol\\Sigma_k^{-1}\\boldsymbol\\Sigma_k = \\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\boldsymbol\\Sigma_k\\boldsymbol\\Sigma_k^{-1}(\\textbf{x}_i-\\boldsymbol\\mu_k)(\\textbf{x}_i-\\boldsymbol\\mu_k)^T \\boldsymbol\\Sigma_k^{-1}\\boldsymbol\\Sigma_k$$\n",
    "\n",
    "$$\\Leftrightarrow \\boldsymbol\\Sigma_k = \\frac{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) (\\textbf{x}_i-\\boldsymbol\\mu_k)(\\textbf{x}_i-\\boldsymbol\\mu_k)^T}{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})}.$$\n",
    "\n",
    "We can replace the the old mean parameters (here $\\boldsymbol\\mu_k = \\boldsymbol\\mu_k^{(t)}$) with the newly found mean parameters $\\boldsymbol\\mu_k^{(t+1)}$, and so we get the next optimal time step covariance matrix as: \n",
    "\n",
    "$$\\boxed{\\boldsymbol\\Sigma_k^{(t+1)} = \\frac{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)}) \\left(\\textbf{x}_i-\\boldsymbol\\mu_k^{(t+1)}\\right)\\left(\\textbf{x}_i-\\boldsymbol\\mu_k^{(t+1)}\\right)^T}{\\sum_{i=1}^n p(y_i=k|\\,\\textbf{x}_i, \\boldsymbol\\theta^{(t)})}}$$\n",
    "\n",
    "The newly updated matrix is a symmetric and positive-semidefinite matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the key steps of the EM-algorithm (upcoming)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the key steps of the EM-algorithm for Gaussian mixture model (upcoming)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python implementation (upcoming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import random, linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def gaussianpdf(mu, sigma, x):\n",
    "    const = 1/np.sqrt(np.power(2*np.pi, len(mu)) * np.linalg.det(sigma))\n",
    "    #print(mu, \"\\n\", sigma, \"\\n\", x)\n",
    "    return const * np.exp(-1/2 * np.dot(np.transpose(x-mu), np.dot(np.linalg.inv(sigma), x-mu)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True taus:\n",
      " [0.38461538461538464, 0.23076923076923078, 0.38461538461538464]\n"
     ]
    }
   ],
   "source": [
    "def createRandomGaussianParams(number_of_dimensions):\n",
    "    mu = random.uniform(0, 15, size=(number_of_dimensions, 1))\n",
    "    A = random.rand(number_of_dimensions, number_of_dimensions)\n",
    "    sigma = np.dot(A,A.transpose())\n",
    "    if random.uniform(0,1) < 0.5:\n",
    "        sigma[0,1] *= -1.0\n",
    "        sigma[1,0] *= -1.0\n",
    "    return mu,sigma\n",
    "\n",
    "def createRandomMixingParams(k):\n",
    "    taus = []\n",
    "    for i in range(k):\n",
    "        if i == k-1:\n",
    "            taus.append(1-np.sum(taus))\n",
    "        else:\n",
    "            tau = random.uniform(0, 1-np.sum(taus))\n",
    "            taus.append(tau)\n",
    "    return taus\n",
    "\n",
    "\n",
    "\n",
    "def createRandomGaussianData(number_of_dimensions, k, sample_list):\n",
    "    mus = np.zeros((number_of_dimensions, k))\n",
    "    covs = np.zeros((number_of_dimensions, number_of_dimensions, k))\n",
    "    samples = []\n",
    "    taus = []\n",
    "    for i in range(k):\n",
    "        mu, sigma = createRandomGaussianParams(number_of_dimensions)\n",
    "        mus[:,i] = mu[:,0]\n",
    "        covs[:,:,i] = sigma\n",
    "        taus.append(sample_list[i]/float(np.sum(sample_list)))\n",
    "        #print(\"The \" + str(i) + \"th mu is: \\n\", mus[:,i])\n",
    "        #print(\"The \" + str(i) + \"th cov is:\\n \", covs[:,:,i], '\\n ********')\n",
    "        sample = np.random.multivariate_normal(mus[:,i], covs[:,:,i],size=sample_list[i])\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return mus, covs, taus, samples\n",
    "\n",
    "# p(y = j|x,theta_t)\n",
    "def membershipValue(mus, covs, taus, x, j):\n",
    "    gaus_j = gaussianpdf(mus[:,j], covs[:,:,j], x) * taus[j]\n",
    "    gaus_sum = 0\n",
    "    for i in range(len(taus)):\n",
    "        gaus_sum += gaussianpdf(mus[:,i], covs[:,:,i], x) * taus[i]\n",
    "    membership_prob = gaus_j / float(gaus_sum)\n",
    "    return membership_prob\n",
    "\n",
    "#def updateParameters():\n",
    "    \n",
    "\n",
    "    #np.random.multivariate_normal(mean, cov,size=5000)\n",
    "mus_true, covs_true, taus_true, samples = createRandomGaussianData(2, 3, [5000, 3000, 5000])\n",
    "#taus = createRandomMixingParams(3)\n",
    "#print(taus, np.sum(taus))\n",
    "#print(\"now\", membershipValue(mus, covs, taus, samples[2][4], 2))\n",
    "\n",
    "mus_est, covs_est, taus_not_needed, samples_not_needed = createRandomGaussianData(2, 3, [1000, 800, 1000])\n",
    "taus_est = createRandomMixingParams(3)\n",
    "\n",
    "\n",
    "## NOW DO THE UPDATES\n",
    "\n",
    "# Apply the update equations here\n",
    "#print(np.concatenate(samples, axis=0 ))\n",
    "sample_matrix = np.concatenate(samples, axis=0)\n",
    "print(\"True taus:\\n\", taus_true)\n",
    "def updateParameters(mus_est, covs_est, taus_est, sample_matrix):\n",
    "    for i in range(len(taus_est)):\n",
    "        membership_sum = 0.0\n",
    "        for data_index in range(sample_matrix.shape[0]):\n",
    "            membership_sum += membershipValue(mus_est, covs_est, taus_est, sample_matrix[data_index,:], i)\n",
    "        taus_est[i] = membership_sum / float(sample_matrix.shape[0])\n",
    "    # Update means\n",
    "    # Update covs\n",
    "    # Plot every update, data, gaussain\n",
    "    # Plot parameter difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [[ 7.33850523 10.04460239  0.03130982]\n",
      " [ 4.78249875  5.89865778  4.26777497]] (2, 3)\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "colors = cm.rainbow(np.linspace(0, 1, mus.shape[1]))\n",
    "print(len(mus), mus, mus.shape)\n",
    "for i in range(mus.shape[1]):\n",
    "    print(len(samples))\n",
    "    plt.scatter(samples[i][:,0], samples[i][:,1], color=colors[i])\n",
    "    sns.kdeplot(samples[i], bw=.15, hue=\"kind\", fill=True)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Update functions. Update mus and covs, create random start --> start updating according to rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
